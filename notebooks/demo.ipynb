{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This demo notebook represents the prototype for the project I am curating. The following section will be small introduction to che prototype itself and later will go over its usage and limitations.\n",
    "## Approach\n",
    "In the beginning of the project, the idea was to individually scrape the complete follower list of a given brand. However, many issues arose:\n",
    "1. There is an inverse correlation between the speed of the scraper and the probability to have the profile banned from instagram;\n",
    "2. The speed of the scraper is determined by the computing power of the device that is used and by the time set to complete a singular action on Instagram;\n",
    "3. In order for the scraper to function perfectly, the connection needs to be constant.\n",
    "After several testings, it appeared clear that the code was not able to reach the entire list of followers of the instagram page. This issue was due to the variable time.sleep (5) set in the beginning. Although the variable served as a shield from a possible Instagram ban, it slowed down the process considerably. As for every user the minimum actions required for the scraping is fuor, and for every action the interval of time is set a five seconds, to collect a total of 10.000 users, the code would have required a total of more that 3.000 hours to complete the process, without any insurance of the stability of the network. For this reason the approach to the project has been changed.\n",
    "The result of the change is the following Scraper Class Demo\n",
    "\n",
    "## Methodology of the code\n",
    "Solving the problem of time  ",
    " The function time.sleep() although functional if utilised results in a predictable pattern of scraping, the human interaction with the platform is much more unpredictable, thus standing out in contrast to the actions of a bot. In order to solve the issue it must be implemented the function frac_lambda() which, similarly to the previous function, utilises lags in the process to simulate a human-like behaviour of the script. The function frac_lambda() applies a time.sleep pattern in which the lag is a variable from 0 to a set number that is randomly assigned to each action.\n",
    "As the problem has been overcome, the issue with the amount of profiles to scrape was still a huge player in determining the outcome of the project. For this reason the approach to the collection of the data has been reshaped, starting from the hypothesis that, in order to be a suitable candidate, a user needs to have had at least one interaction with the page of the brand. In this specific case an interaction is defined as a “like” or a “comment” on at least on of the past three posts of the brand’s page. By utilising the chunk of code created for the first prototype it is possible to start the process of collecting the full list of the followers from the brand’s page, which is saved on the coder’s device permanently as a data-frame (using the library Pandas). The next step consists in creating a second list of the users that “liked” the posts of the brand, using the function scraper.scrape_post_data(1, max_likes = 450), where the first number in the brackets represents the number of the post, and the max_likes variable is used to set a limit for the scraper in order to avoid overlaps in the users.\n",
    "\n",
    "Once the two lists have been created, the results will be cross-checked and a third and final list of users who are both followers and have liked the past posts will be created. This final data-frame represents the more suitable users. However, so far no profiling on the users has been performed. Thus, the next step of the process will be creating, with the use of pre-existing literature, indicators that will help in selecting the correct sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper Class Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives a walk through demo of the insta-scraper class. The notebook is not exhaustive, but all functions are documented. Please run this notebook from within the cloned git repository. The first step is to import the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from insta_scraper import scraper as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = s.insta_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code was structured to complete the following task: control the search engine in incognito mode, via a Phyton library called Selenium (selenium.webdriver.common_action_chains); access Instagram using the function “getpass” and by using a burner account; targeting the instagram page of the selected brand, using the function def take_me_to(self, ‘url’); And finally, reach the follower section a start the scraping using def get_followers (self, max_followers, show=false) and def get_followers_info (self, num_post, show=false).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initiate the webdrivere select the following chunk and press on the play button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start web driver and perform login. A path a valid path to driver must be supplied. This automatically performs insta login; a valid user name and password must be supplied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "insta username:  testburner25\n",
      "insta password:  ········\n"
     ]
    }
   ],
   "source": [
    "scraper.start_driver(\"../webdriver/win/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the list of accounts following our target account manually as the list takes a long time to collect. \n",
    "We set the list of accounts following our target account manually as the list takes a long time to collect. Collecting the list can be achieved by using a third party app (like PhantomBuster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.instagram.com/saurabhgupta07',\n",
       " 'https://www.instagram.com/johannydavalillo',\n",
       " 'https://www.instagram.com/yskandartakchi',\n",
       " 'https://www.instagram.com/hosseinkarimian01',\n",
       " 'https://www.instagram.com/djazairiaa_5',\n",
       " 'https://www.instagram.com/david3011',\n",
       " 'https://www.instagram.com/rexelpoland',\n",
       " 'https://www.instagram.com/gpope83',\n",
       " 'https://www.instagram.com/elena_antunes_670']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.set_follower_list(\"../data/dopper_official.csv\", header = None, col_index = 0)\n",
    "scraper.target_follower_list[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we navigate to the target account using the insta handle of the account we are interested in scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.target_accout(\"dopper_official\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this also updates the target account meta data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dopper_official\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'post': '1283', 'followers': '24.5k', 'following': '1558'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scraper.target_handle)\n",
    "scraper.target_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define and interpret the data that will be scraped, it is needed a comprehensive ranking system that takes into consideration each aspect of the user’s profile. The code used offers three cycles of iteration with the data in which the users will be ranked using a tokenisation system, if the user scores positively in a certain category a unit will be added to its overall score. Each category has a specific weight towards the final result in order to value more the calories that are more relevant toward the accomplishment of the project and value less the categories that might enhance the performances of unwanted users.\n",
    "\n",
    "We first set the scraper to collect users interacting with post 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=6\n",
    "for i in range(6):\n",
    "scraper.scrape_post_data(i+1, max_likes= float('inf')) time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examin the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follows_target</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>n_following</th>\n",
       "      <th>n_posts</th>\n",
       "      <th>posts_over_time</th>\n",
       "      <th>bio</th>\n",
       "      <th>post_1_content</th>\n",
       "      <th>post_2_content</th>\n",
       "      <th>post_3_content</th>\n",
       "      <th>likes_post_1</th>\n",
       "      <th>comments_post_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>geis.th</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nickspapens</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_.mam.v.pici._</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bianca_loves_efteling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nils.0412</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      follows_target n_followers n_following n_posts  \\\n",
       "geis.th                            0           0           0       0   \n",
       "nickspapens                        0           0           0       0   \n",
       "_.mam.v.pici._                     0           0           0       0   \n",
       "bianca_loves_efteling              0           0           0       0   \n",
       "nils.0412                          0           0           0       0   \n",
       "\n",
       "                      posts_over_time bio post_1_content post_2_content  \\\n",
       "geis.th                             0   0              0              0   \n",
       "nickspapens                         0   0              0              0   \n",
       "_.mam.v.pici._                      0   0              0              0   \n",
       "bianca_loves_efteling               0   0              0              0   \n",
       "nils.0412                           0   0              0              0   \n",
       "\n",
       "                      post_3_content  likes_post_1  comments_post_1  \n",
       "geis.th                            0             1                0  \n",
       "nickspapens                        0             1                0  \n",
       "_.mam.v.pici._                     0             1                0  \n",
       "bianca_loves_efteling              0             1                0  \n",
       "nils.0412                          0             1                0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 users have interacted with the posts scraper so far\n"
     ]
    }
   ],
   "source": [
    "print(\"{} users have interacted with the posts scraper so far\".format(len(scraper.df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User data preprocessing\n",
    "\n",
    "The scraper will collect the following data, if available, meaning that the profile privacy setting is set on public:\n",
    "Number of Followers Number of Following Number of Posts Biography\n",
    "Captions of the first three posts of the user Comments of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.scrape_user_data(frac_lambda=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling \n",
    "In the unfortunate occasion where instagram kick us out of the platform: Write data to disk and leave..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " scraper.df.to_csv(\"temp_data.csv\") scraper.quit_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the notebook has been re-started run this chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(\"../\")\n",
    "from insta_scraper import scraper as s\n",
    "scraper = s.insta_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log back in to Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.start_driver(\"../webdriver/mac/chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if the notebook has been restarted run this chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "scraper.set_follower_list(\"../data/dopper_official_followers_list.csv\", header = None, col_index = 1) \n",
    "# set follower list again scraper.target_accout(\"dopper_official\") \n",
    "# target the account again... \n",
    "scraper.df = pd.read_csv(\"temp_data.csv\", index_col=0) # set temp data AGAIN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...pick up from where you left off (perhaps try to set the variable frac_lambda to a value higher than 5)\n",
    "later visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraoer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving User Data\n",
    "Storing the information we have just collected can be achieved by running the following chunks of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.df.to_csv(\"complete_dataframe.csv\") \n",
    "scraper.df.to_excel(\"complete_dataframe.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Natural Language Processing Techniques\n",
    "The techniques utilized in this final segment are tokenization and word occurence strategies, in both cases the overall approach follows text analysis operations\n",
    "\n",
    "### Sentiment Analysis\n",
    "The sentiment analysis proposed does not necessarily follow the parameters set for base sentiment analyses. The reason behind this choice can be found on the computing power of the device utilised as well as the impossibility to create a data frame used for the training of the algorithm. Nonetheless, the goal of the sentiment analysis still distinguishes positive and negative words. In order for the process to function correctly, the code needs to cross-check all the words scraped from a user’s profile and confront them with a list of positive and negative words. The list reaches a total of over 6.000 words. Both negative and positive opinion lexicons are used to execute an occurrence text analysis, to avoid possible exclusion of misspelled words by the users, many misspelled words have been added to the list.\n",
    "### Maslow Pyramid of needs to assess user's Personality\n",
    "In order to give more depth to the analysis of the data, by applying the concepts of the Maslow’s Pyramid of Needs model, it possible to highlight the degree of relevance of each user’s content. Moreover, the words connected to each category have been added using the app TagsFinder.com, a software that enables the user to collect most used hashtags, as well as creating related hashtags to certain topics of main interest. In order to appropriately address the selection of the words, as the main topic of reference, the Maslow dimension has been selected. Furthermore, the created hashtags have been used as keywords in the text analysis. Finally, a score to each category of the Maslow’s pyramid has been assigned, taking into consideration the fact that higher categories needs to have a higher degree of relevancy towards the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.word_occurence_analysis(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iorder to visualize a first preview of the data, we are utilizing the following chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " scraper.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Final Results\n",
    "Exporting the final results can be achieved by running the following chunk of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.df.to_excel(\"FinalDocument.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
